{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it4omyg5G3gm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "df.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeWhttz6Ku1n",
        "outputId": "afa849cb-4f54-44be-eb96-b3485df0d8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object DataFrame.items at 0x7ced12323ae0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.URL_ID[8]"
      ],
      "metadata": {
        "id": "zijE2JKzPsgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "WH_2FeXPLMGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_and_save(url_id, url,target_directory):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1').get_text()\n",
        "        # print(title)\n",
        "        text = soup.find('div', class_= 'td-post-content')\n",
        "\n",
        "        if text:\n",
        "          pre_tag = text.find('pre')\n",
        "          if pre_tag:\n",
        "            pre_tag.decompose()\n",
        "\n",
        "        new_text = text.get_text()\n",
        "        print(new_text)\n",
        "\n",
        "        file_path = os.path.join(target_directory, f'{url_id}.txt')\n",
        "\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(f'Title: {title}\\n\\n')\n",
        "            file.write(f'Text: {new_text}')\n",
        "        print(f'Saved {url_id}.txt')\n",
        "    else:\n",
        "        print(f'Failed to retrieve data from {url_id}')"
      ],
      "metadata": {
        "id": "dFZ3gNChOAEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url_id = df.URL_ID[8]\n",
        "# url = df.URL[8]\n",
        "# scrape_and_save(url_id,url,'/content/Output')"
      ],
      "metadata": {
        "id": "-jxtpnbT9NNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    scrape_and_save(url_id, url, '/content/Output')"
      ],
      "metadata": {
        "id": "ALNgYSFjPdCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}